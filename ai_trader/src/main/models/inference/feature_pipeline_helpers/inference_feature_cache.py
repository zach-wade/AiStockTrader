# File: src/ai_trader/models/inference/feature_pipeline_helpers/inference_feature_cache.py

import logging
import json # For serializing cache keys if complex objects
import hashlib
from datetime import datetime, timedelta, timezone
from typing import Dict, Any, Optional, Tuple, List
import asyncio

from main.utils.cache import MemoryBackend
from main.utils.cache import CacheType

logger = logging.getLogger(__name__)

class InferenceFeatureCache:
    """
    Manages caching for computed feature sets to avoid redundant
    calculations within a short time window. Designed for real-time inference.
    Now uses MarketDataCache for unified caching across the system.
    """

    def __init__(self, cache_ttl_seconds: Optional[int] = None):
        """
        Initializes the InferenceFeatureCache.

        Args:
            cache_ttl_seconds: Time-to-live for cached feature results in seconds.
                              If None, will use config default.
        """
        # Use config default or provided value
        if cache_ttl_seconds is None:
            from main.config.config_manager import get_config
            config = get_config()
            self._cache_ttl_seconds = config.get('cache', {}).get('ttl', {}).get('features', 300)
        else:
            self._cache_ttl_seconds = cache_ttl_seconds
            
        self._cache_instance = None
        
        # Statistics
        self._cache_hits = 0
        self._cache_misses = 0
        
        logger.debug(f"InferenceFeatureCache initialized with TTL: {self._cache_ttl_seconds}s, using MarketDataCache")
    
    def _get_cache_instance(self):
        """Get cache instance with lazy initialization."""
        if self._cache_instance is None:
            self._cache_instance = get_global_cache()
        return self._cache_instance

    def get_cache_key(self, symbol: str, features_list: List[str], interval: str = '1min') -> str:
        """
        Generates a consistent cache key for a set of features for a symbol and interval.
        Feature list is sorted for consistent key generation.

        Args:
            symbol: The stock symbol.
            features_list: The list of features requested.
            interval: The time interval of the data (e.g., '1min', '1hour').

        Returns:
            A string representing the cache key.
        """
        sorted_features = sorted(features_list)
        # Use a combination of symbol, interval, and sorted features to form the key
        # Using a hash for features_list to keep key length manageable if features_list is long
        features_hash = hashlib.md5(json.dumps(sorted_features).encode('utf-8')).hexdigest()
        return f"{symbol}_{interval}_{features_hash}"

    async def get_cached_features(self, cache_key: str) -> Optional[Dict[str, float]]:
        """
        Retrieves computed features from the cache if available and not expired.

        Args:
            cache_key: The key generated by `get_cache_key`.

        Returns:
            A dictionary of feature values, or None if not found or expired.
        """
        try:
            cache = self._get_cache_instance()
            cached_result = await cache.get(CacheType.FEATURES, cache_key)
            
            if cached_result is not None:
                self._cache_hits += 1
                logger.debug(f"Cache hit for features: {cache_key}")
                return cached_result
            else:
                self._cache_misses += 1
                logger.debug(f"Cache miss for features: {cache_key}")
                return None
        except Exception as e:
            logger.warning(f"Cache get failed for key {cache_key}: {e}")
            self._cache_misses += 1
            return None

    async def set_cached_features(self, cache_key: str, features_dict: Dict[str, float]):
        """
        Caches computed features with a defined time-to-live.

        Args:
            cache_key: The key generated by `get_cache_key`.
            features_dict: A dictionary of feature names and their computed float values.
        """
        try:
            cache = self._get_cache_instance()
            success = await cache.set(CacheType.FEATURES, cache_key, features_dict, self._cache_ttl_seconds)
            
            if success:
                logger.debug(f"Cached features for key: {cache_key}")
            else:
                logger.warning(f"Failed to cache features for key: {cache_key}")
        except Exception as e:
            logger.warning(f"Cache set failed for key {cache_key}: {e}")

    async def clear_cache(self):
        """Clears the entire feature cache."""
        try:
            cache = self._get_cache_instance()
            success = await cache.clear(CacheType.FEATURES)
            if success:
                logger.info("Inference feature cache cleared.")
            else:
                logger.warning("Failed to clear inference feature cache.")
        except Exception as e:
            logger.warning(f"Cache clear failed: {e}")

    def get_cache_size(self) -> int:
        """Returns cache statistics since MarketDataCache doesn't expose size directly."""
        return self._cache_hits + self._cache_misses
    
    def get_cache_stats(self) -> Dict[str, Any]:
        """Returns cache statistics."""
        total_requests = self._cache_hits + self._cache_misses
        return {
            'cache_hits': self._cache_hits,
            'cache_misses': self._cache_misses,
            'hit_rate': (self._cache_hits / total_requests * 100) if total_requests > 0 else 0,
            'total_requests': total_requests
        }