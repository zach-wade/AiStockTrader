---
name: Full Quality Gates CI

'on':
  schedule:
    # Run nightly at 2 AM UTC
    - cron: '0 2 * * *'
  workflow_dispatch:
    inputs:
      generate_report:
        description: 'Generate detailed quality report'
        required: false
        type: boolean
        default: true

jobs:
  full-quality-gates:
    runs-on: ubuntu-latest
    timeout-minutes: 30
    continue-on-error: true  # Don't fail the workflow, just report

    strategy:
      matrix:
        python-version: ["3.11", "3.12"]

    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 0  # Full history for analysis

      - name: Set up Python ${{ matrix.python-version }}
        uses: actions/setup-python@v5
        with:
          python-version: ${{ matrix.python-version }}

      - name: Cache pip packages
        uses: actions/cache@v3
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ matrix.python-version }}-${{ hashFiles('requirements*.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-${{ matrix.python-version }}-

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install -r requirements-dev.txt

      - name: Run Black formatter check
        continue-on-error: true
        run: |
          echo "### Black Formatting" >> quality_report.md
          if black --check --diff .; then
            echo "✅ Code formatting is correct" >> quality_report.md
          else
            echo "❌ Code formatting issues found" >> quality_report.md
            black --diff . >> formatting_issues.txt 2>&1
          fi

      - name: Run Ruff linter
        continue-on-error: true
        run: |
          echo "### Ruff Linting" >> quality_report.md
          if ruff check .; then
            echo "✅ No linting issues" >> quality_report.md
          else
            echo "❌ Linting issues found" >> quality_report.md
            ruff check . --output-format=json > ruff_report.json 2>&1 || true
          fi

      - name: Run MyPy type checker
        continue-on-error: true
        run: |
          echo "### MyPy Type Checking" >> quality_report.md
          mypy src --ignore-missing-imports --show-error-codes > mypy_report.txt 2>&1 || true
          ERROR_COUNT=$(grep -c "error:" mypy_report.txt || echo "0")
          echo "Type errors: $ERROR_COUNT" >> quality_report.md

      - name: Run security scans
        continue-on-error: true
        run: |
          echo "### Security Scanning" >> quality_report.md

          # Bandit
          bandit -r src -ll -f json -o bandit_report.json || true

          # Safety check
          safety check -r requirements.txt --json > safety_report.json 2>&1 || true

          # pip-audit
          pip-audit -r requirements.txt --format json > audit_report.json 2>&1 || true

          echo "Security reports generated" >> quality_report.md

      - name: Run ALL tests with coverage
        continue-on-error: true
        run: |
          echo "### Test Results" >> quality_report.md

          PYTHONPATH=. pytest tests/ \
            --cov=src \
            --cov-report=xml \
            --cov-report=term \
            --cov-report=html \
            --junit-xml=test_results.xml \
            -v || true

          # Extract test statistics
          TOTAL_TESTS=$(grep -oP 'collected \K\d+' test_output.txt || echo "0")
          PASSED=$(grep -oP '\d+(?= passed)' test_output.txt || echo "0")
          FAILED=$(grep -oP '\d+(?= failed)' test_output.txt || echo "0")
          SKIPPED=$(grep -oP '\d+(?= skipped)' test_output.txt || echo "0")

          echo "Total: $TOTAL_TESTS | Passed: $PASSED | Failed: $FAILED | Skipped: $SKIPPED" >> quality_report.md

      - name: Calculate quality metrics
        continue-on-error: true
        run: |
          python -c "
          import json
          import xml.etree.ElementTree as ET

          # Parse coverage
          try:
              tree = ET.parse('coverage.xml')
              root = tree.getroot()
              coverage = float(root.attrib.get('line-rate', 0)) * 100
          except:
              coverage = 0

          # Count MyPy errors
          try:
              with open('mypy_report.txt', 'r') as f:
                  mypy_errors = sum(1 for line in f if 'error:' in line)
          except:
              mypy_errors = 0

          # Create metrics
          metrics = {
              'coverage': f'{coverage:.2f}%',
              'mypy_errors': mypy_errors,
              'python_version': '${{ matrix.python-version }}'
          }

          with open('metrics.json', 'w') as f:
              json.dump(metrics, f, indent=2)

          print(f'Coverage: {coverage:.2f}%')
          print(f'MyPy Errors: {mypy_errors}')
          "

      - name: Generate quality report
        if: always()
        run: |
          echo "# 📊 Full Quality Report - Python ${{ matrix.python-version }}" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          # Add timestamp
          echo "**Generated:** $(date -u '+%Y-%m-%d %H:%M:%S UTC')" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          # Add metrics if available
          if [ -f metrics.json ]; then
            echo "## Key Metrics" >> $GITHUB_STEP_SUMMARY
            echo '```json' >> $GITHUB_STEP_SUMMARY
            cat metrics.json >> $GITHUB_STEP_SUMMARY
            echo '```' >> $GITHUB_STEP_SUMMARY
          fi

          # Add quality report content
          if [ -f quality_report.md ]; then
            cat quality_report.md >> $GITHUB_STEP_SUMMARY
          fi

          echo "" >> $GITHUB_STEP_SUMMARY
          echo "## Progress Tracking" >> $GITHUB_STEP_SUMMARY
          echo "- 🎯 Target: 100% tests passing, 0 MyPy errors, 80% coverage" >> $GITHUB_STEP_SUMMARY
          echo "- 📈 Track progress in artifacts and GitHub Actions summary" >> $GITHUB_STEP_SUMMARY

      - name: Upload quality artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: quality-report-${{ matrix.python-version }}
          path: |
            quality_report.md
            mypy_report.txt
            test_results.xml
            coverage.xml
            htmlcov/
            bandit_report.json
            safety_report.json
            audit_report.json
            metrics.json
            ruff_report.json

  quality-summary:
    needs: full-quality-gates
    runs-on: ubuntu-latest
    if: always()

    steps:
      - name: Download artifacts
        uses: actions/download-artifact@v4
        with:
          pattern: quality-report-*
          merge-multiple: true

      - name: Create consolidated report
        run: |
          echo "# 🏆 Consolidated Quality Report" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "Full quality gate analysis completed." >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          # List all quality issues to track
          echo "## Known Issues to Fix" >> $GITHUB_STEP_SUMMARY
          echo "- [ ] 816 unit test failures" >> $GITHUB_STEP_SUMMARY
          echo "- [ ] 264 MyPy type errors" >> $GITHUB_STEP_SUMMARY
          echo "- [ ] Coverage below 80% target" >> $GITHUB_STEP_SUMMARY
          echo "- [ ] Portfolio entity SOLID violations" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          echo "Check artifacts for detailed reports." >> $GITHUB_STEP_SUMMARY
