# Validation System Data Flow Boundaries

This document provides a clear visualization of data flow through the multi-stage validation system, including boundaries, checkpoints, and decision points.

## Data Flow Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  External Data  â”‚
â”‚    Sources      â”‚
â”‚  â€¢ Alpaca       â”‚
â”‚  â€¢ Polygon      â”‚
â”‚  â€¢ Yahoo        â”‚
â”‚  â€¢ Reddit       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                    BOUNDARY 1: INGEST                         â•‘
â•Ÿâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¢
â•‘ â€¢ Raw data enters the system                                  â•‘
â•‘ â€¢ Source-specific format validation                           â•‘
â•‘ â€¢ Schema validation                                           â•‘
â•‘ â€¢ Required fields check                                       â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                        â”‚
                        â–¼
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚ Ingest Stage  â”‚â”€â”€â”€â”€â”€â”€â–º âŒ DROP_ROW (on ERROR)
                â”‚  Validation   â”‚â”€â”€â”€â”€â”€â”€â–º âš ï¸ FLAG_ROW (on WARNING)
                â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â–¼
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚   Raw Data    â”‚
                â”‚    Store      â”‚
                â”‚ (Data Lake)   â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â–¼
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚     ETL       â”‚
                â”‚  Processing   â”‚
                â”‚ â€¢ Transform   â”‚
                â”‚ â€¢ Aggregate   â”‚
                â”‚ â€¢ Standardize â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â–¼
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                   BOUNDARY 2: POST-ETL                        â•‘
â•Ÿâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¢
â•‘ â€¢ Transformed data validation                                 â•‘
â•‘ â€¢ Aggregation correctness                                     â•‘
â•‘ â€¢ Data consistency checks                                     â•‘
â•‘ â€¢ Business rules validation                                   â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                        â”‚
                        â–¼
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚ Post-ETL      â”‚â”€â”€â”€â”€â”€â”€â–º âŒ SKIP_SYMBOL (on ERROR)
                â”‚ Validation    â”‚â”€â”€â”€â”€â”€â”€â–º âš ï¸ FLAG_AND_CONTINUE (on WARNING)
                â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜        ðŸ“¢ ALERT (on ERROR)
                        â”‚
                        â–¼
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚  Processed    â”‚
                â”‚  Data Store   â”‚
                â”‚ (Hot Storage) â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â–¼
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                BOUNDARY 3: FEATURE-READY                      â•‘
â•Ÿâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¢
â•‘ â€¢ Feature calculation prerequisites                           â•‘
â•‘ â€¢ Sufficient history check                                    â•‘
â•‘ â€¢ Statistical validity                                        â•‘
â•‘ â€¢ Data completeness                                          â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                        â”‚
                        â–¼
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚ Feature-Ready â”‚â”€â”€â”€â”€â”€â”€â–º âŒ USE_LAST_GOOD (on ERROR)
                â”‚  Validation   â”‚â”€â”€â”€â”€â”€â”€â–º âš ï¸ CONTINUE_WITH_WARNING
                â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜        ðŸ“¢ ALERT (on ERROR)
                        â”‚
                        â–¼
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚   Feature     â”‚
                â”‚  Engineering  â”‚
                â”‚   Pipeline    â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â–¼
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚   Features    â”‚
                â”‚    Store      â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Detailed Boundary Specifications

### Boundary 1: INGEST

**Purpose**: Protect the system from malformed or invalid raw data

#### Input Contract

- **Format**: JSON, CSV, or API response objects
- **Sources**: External data providers (Alpaca, Polygon, Yahoo, Reddit)
- **Volume**: Variable (1-1M records per batch)

#### Validation Rules

```yaml
Required Checks:
  - Schema validation against source-specific formats
  - Field presence (timestamp, OHLCV for market data)
  - Data type validation (numeric prices, valid timestamps)
  - Basic range checks (prices > 0, volumes >= 0)

Actions on Failure:
  - ERROR â†’ DROP_ROW (protect downstream)
  - WARNING â†’ FLAG_ROW (allow but track)
```

#### Output Contract

- **Format**: Validated raw data
- **Guarantees**:
  - All required fields present
  - Basic data types correct
  - No null prices
- **Metadata**: Source, validation_timestamp, flags

### Boundary 2: POST-ETL

**Purpose**: Ensure data transformations maintain integrity

#### Input Contract

- **Format**: Pandas DataFrame or structured data
- **Source**: ETL pipeline output
- **Assumptions**: Already passed INGEST validation

#### Validation Rules

```yaml
Required Checks:
  - OHLC relationship integrity (High >= Low, etc.)
  - Time series consistency
  - Aggregation accuracy
  - No missing periods (configurable tolerance)
  - Statistical outlier detection

Actions on Failure:
  - ERROR â†’ SKIP_SYMBOL + ALERT
  - WARNING â†’ FLAG_AND_CONTINUE
```

#### Output Contract

- **Format**: Standardized DataFrame
- **Guarantees**:
  - Consistent time intervals
  - Valid OHLC relationships
  - No extreme outliers
  - Complete time series
- **Metadata**: aggregation_level, validation_profile, quality_score

### Boundary 3: FEATURE-READY

**Purpose**: Ensure data meets feature calculation requirements

#### Input Contract

- **Format**: Standardized DataFrame
- **Source**: Processed data store
- **Assumptions**: Passed POST-ETL validation

#### Validation Rules

```yaml
Required Checks:
  - Minimum history (e.g., 252 days for annual features)
  - Column completeness
  - No lookahead bias
  - Stationarity tests (for ML features)
  - Cross-sectional consistency

Actions on Failure:
  - ERROR â†’ USE_LAST_GOOD_SNAPSHOT + ALERT
  - WARNING â†’ CONTINUE_WITH_WARNING
```

#### Output Contract

- **Format**: Feature-ready DataFrame
- **Guarantees**:
  - Sufficient history for all calculations
  - All required columns present
  - Data statistically valid
  - No temporal leakage
- **Metadata**: feature_readiness_score, warnings

## Decision Trees at Each Boundary

### INGEST Decision Tree

```
Data Arrives
    â”‚
    â”œâ”€ Schema Valid?
    â”‚   â”œâ”€ NO â†’ DROP_ROW
    â”‚   â””â”€ YES â†“
    â”‚
    â”œâ”€ Required Fields Present?
    â”‚   â”œâ”€ NO â†’ DROP_ROW
    â”‚   â””â”€ YES â†“
    â”‚
    â”œâ”€ Data Types Correct?
    â”‚   â”œâ”€ NO â†’ FLAG_ROW + Try Conversion
    â”‚   â”‚       â”œâ”€ Success â†’ CONTINUE
    â”‚   â”‚       â””â”€ Fail â†’ DROP_ROW
    â”‚   â””â”€ YES â†“
    â”‚
    â””â”€ Business Rules Pass?
        â”œâ”€ NO â†’ DROP_ROW (if ERROR)
        â”‚       FLAG_ROW (if WARNING)
        â””â”€ YES â†’ PASS TO ETL
```

### POST-ETL Decision Tree

```
Transformed Data
    â”‚
    â”œâ”€ OHLC Relationships Valid?
    â”‚   â”œâ”€ NO â†’ SKIP_SYMBOL + ALERT
    â”‚   â””â”€ YES â†“
    â”‚
    â”œâ”€ Time Consistency OK?
    â”‚   â”œâ”€ NO â†’ Check Gap Size
    â”‚   â”‚       â”œâ”€ > Threshold â†’ SKIP_SYMBOL
    â”‚   â”‚       â””â”€ < Threshold â†’ FLAG + INTERPOLATE
    â”‚   â””â”€ YES â†“
    â”‚
    â”œâ”€ Outliers Detected?
    â”‚   â”œâ”€ YES â†’ Check Severity
    â”‚   â”‚        â”œâ”€ Extreme â†’ SKIP_SYMBOL
    â”‚   â”‚        â””â”€ Moderate â†’ FLAG_AND_CONTINUE
    â”‚   â””â”€ NO â†“
    â”‚
    â””â”€ Quality Score
        â”œâ”€ < 0.8 â†’ WARNING + CONTINUE
        â””â”€ >= 0.8 â†’ PASS TO STORAGE
```

### FEATURE-READY Decision Tree

```
Pre-Feature Data
    â”‚
    â”œâ”€ Sufficient History?
    â”‚   â”œâ”€ NO â†’ USE_LAST_GOOD_SNAPSHOT
    â”‚   â”‚       â”œâ”€ Available & Fresh â†’ USE IT
    â”‚   â”‚       â””â”€ Not Available â†’ SKIP_CALCULATION
    â”‚   â””â”€ YES â†“
    â”‚
    â”œâ”€ All Columns Present?
    â”‚   â”œâ”€ NO â†’ Check if Optional
    â”‚   â”‚       â”œâ”€ Required â†’ FAIL
    â”‚   â”‚       â””â”€ Optional â†’ CONTINUE_WITH_WARNING
    â”‚   â””â”€ YES â†“
    â”‚
    â”œâ”€ Statistical Tests Pass?
    â”‚   â”œâ”€ NO â†’ Log Details
    â”‚   â”‚       â”œâ”€ Critical â†’ USE_LAST_GOOD
    â”‚   â”‚       â””â”€ Non-critical â†’ WARNING
    â”‚   â””â”€ YES â†“
    â”‚
    â””â”€ Final Check
        â””â”€ PASS TO FEATURE PIPELINE
```

## Data Quality Metrics at Boundaries

### Boundary Metrics

| Metric | INGEST | POST-ETL | FEATURE-READY |
|--------|--------|----------|---------------|
| **Schema Compliance** | âœ“ | - | - |
| **Completeness** | âœ“ | âœ“ | âœ“ |
| **Accuracy** | Basic | âœ“ | âœ“ |
| **Consistency** | - | âœ“ | âœ“ |
| **Timeliness** | âœ“ | âœ“ | - |
| **Statistical Validity** | - | âœ“ | âœ“ |

### Quality Score Calculation

```python
def calculate_quality_score(validation_results):
    """
    Calculate overall data quality score at each boundary
    """
    weights = {
        'completeness': 0.3,
        'accuracy': 0.3,
        'consistency': 0.2,
        'timeliness': 0.1,
        'validity': 0.1
    }

    scores = {
        'completeness': 1 - (missing_data_pct / 100),
        'accuracy': 1 - (error_count / total_count),
        'consistency': consistency_check_pass_rate,
        'timeliness': 1 - (data_age_hours / 24),
        'validity': statistical_test_pass_rate
    }

    return sum(weights[k] * scores[k] for k in weights)
```

## Integration Points

### With Monitoring Systems

```yaml
Prometheus Metrics:
  - validation_success_rate{stage="ingest", source="alpaca"}
  - validation_errors_total{stage="post_etl", error_type="ohlc_violation"}
  - data_quality_score{stage="feature_ready", symbol="AAPL"}
  - validation_duration_seconds{stage="ingest"}
```

### With Alerting Systems

```yaml
Alert Rules:
  - name: High Validation Failure Rate
    condition: validation_success_rate < 0.95
    stages: [post_etl, feature_ready]
    severity: warning

  - name: Critical Data Quality Issue
    condition: validation_success_rate < 0.80
    stages: all
    severity: critical

  - name: Data Staleness
    condition: hours_since_last_valid_data > 2
    stages: [ingest]
    severity: warning
```

### With Data Lineage

```yaml
Lineage Tracking:
  - validation_id: UUID for each validation run
  - parent_validation_id: Link to previous stage
  - transformations_applied: List of ETL operations
  - quality_impact: How validation affected quality score
```

## Best Practices

1. **Fail Fast**: Catch issues at the earliest boundary
2. **Clear Contracts**: Define explicit input/output expectations
3. **Graceful Degradation**: Use fallbacks when appropriate
4. **Audit Trail**: Log all validation decisions
5. **Performance Balance**: Profile-based validation for different use cases
6. **Monitoring**: Track metrics at each boundary
7. **Alerting**: Set up appropriate alerts for critical failures
